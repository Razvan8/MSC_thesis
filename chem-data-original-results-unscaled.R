#######################
## Data processing
#######################

# Plate 1.1
plate1.1 <- read.csv("yield_data\\plate1.1.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate1.1_pdt <- plate1.1$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate1.1_pdt))
dim(plate.data) <- c(24,16)
plate.data1.1 <- t(plate.data)

# Plate 1.2
plate1.2 <- read.csv("yield_data\\plate1.2.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate1.2_pdt <- plate1.2$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate1.2_pdt))
dim(plate.data) <- c(24,16)
plate.data1.2 <- t(plate.data)

# Plate 1.3
plate1.3 <- read.csv("yield_data\\plate1.3.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate1.3_pdt <- plate1.3$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate1.3_pdt))
dim(plate.data) <- c(24,16)
plate.data1.3 <- t(plate.data)

# Plate 1.4
plate1.4 <- read.csv("yield_data\\plate1.4.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate1.4_pdt <- plate1.4$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate1.4_pdt))
dim(plate.data) <- c(24,16)
plate.data1.4 <- t(plate.data)

# stitch Plate 1 together into one 32x48 matrix
plate1.top <- cbind(plate.data1.1, plate.data1.2)
plate1.bottom <- cbind(plate.data1.3, plate.data1.4)
plate1 <- rbind(plate1.top, plate1.bottom)

# Plate 2.1
plate2.1 <- read.csv("yield_data\\plate2.1.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate2.1_pdt <- plate2.1$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate2.1_pdt))
dim(plate.data) <- c(24,16)
plate.data2.1 <- t(plate.data)

# Plate 2.2
plate2.2 <- read.csv("yield_data\\plate2.2.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate2.2_pdt <- plate2.2$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate2.2_pdt))
dim(plate.data) <- c(24,16)
plate.data2.2 <- t(plate.data)

# Plate 2.3
plate2.3 <- read.csv("yield_data\\plate2.3.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate2.3_pdt <- plate2.3$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate2.3_pdt))
dim(plate.data) <- c(24,16)
plate.data2.3 <- t(plate.data)

# Plate 2.4
plate2.4 <- read.csv("yield_data\\plate2.4.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate2.4_pdt <- plate2.4$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate2.4_pdt))
dim(plate.data) <- c(24,16)
plate.data2.4 <- t(plate.data)

# stitch Plate 2 together into one 32x48 matrix
plate2.top <- cbind(plate.data2.1, plate.data2.2)
plate2.bottom <- cbind(plate.data2.3, plate.data2.4)
plate2 <- rbind(plate2.top, plate2.bottom)

# Plate 3.1
plate3.1 <- read.csv("yield_data\\plate3.1.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate3.1_pdt <- plate3.1$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate3.1_pdt))
dim(plate.data) <- c(24,16)
plate.data3.1 <- t(plate.data)

# Plate 3.2
plate3.2 <- read.csv("yield_data\\plate3.2.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate3.2_pdt <- plate3.2$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate3.2_pdt))
dim(plate.data) <- c(24,16)
plate.data3.2 <- t(plate.data)

# Plate 3.3
plate3.3 <- read.csv("yield_data\\plate3.3.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate3.3_pdt <- plate3.3$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate3.3_pdt))
dim(plate.data) <- c(24,16)
plate.data3.3 <- t(plate.data)

# Plate 3.4
plate3.4 <- read.csv("yield_data\\plate3.4.csv", header=TRUE, stringsAsFactors=FALSE, na.strings = "#DIV/0!")
plate3.4_pdt <- plate3.4$product_scaled[1:384]
plate.data <- as.matrix(as.numeric(plate3.4_pdt))
dim(plate.data) <- c(24,16)
plate.data3.4 <- t(plate.data)

# stitch Plate 3 together into one 32x48 matrix
plate3.top <- cbind(plate.data3.1, plate.data3.2)
plate3.bottom <- cbind(plate.data3.3, plate.data3.4)
plate3 <- rbind(plate3.top, plate3.bottom)

# Remove empty rows/cols
plate1_nocontrols <- plate1[c(-1,-5,-9,-13,-20,-24,-28,-32), c(-16,-32,-48)] 
plate2_nocontrols <- plate2[, c(-16,-32,-48)]
plate3_nocontrols <- plate3[, c(-16,-32,-48)]
plate1_nocontrols_v <- as.vector(t(plate1_nocontrols))
plate2_nocontrols_v <- as.vector(t(plate2_nocontrols))
plate3_nocontrols_v <- as.vector(t(plate3_nocontrols))
yield_data <- c(plate1_nocontrols_v, plate2_nocontrols_v, plate3_nocontrols_v)

# Load output table generated by python script
output.table <- read.csv("yield_data\\output_table.csv", header=TRUE)

# Scaled variables
y <- as.data.frame(yield_data[-which(is.na(yield_data))]/100)
x <- as.data.frame(output.table[-which(is.na(yield_data)),])
#x <- as.data.frame(scale(output.table[-which(is.na(yield_data)),]))
tt <- (1:length(y[,1]))/length(y)

# Add three artificial descriptors for additives
xc <- x
set.seed(2134)
c1 <- factor(xc[,1])
c2 <- factor(xc[,3])
c3 <- factor(xc[,4])
levels(c1) <- runif(22)
levels(c2) <- rnorm(22)
levels(c3) <- runif(22)
xc <- cbind(cbind(as.numeric(c1),as.numeric(c2),as.numeric(c3)),xc)
colnames(xc)[1:3] <- c("add_new1","add_new2","add_new3")

xs <- xc[,c(4,23,50,60)]
colnames(xs) <- c("additive","aryl_halide","base","ligand")

a <- rep(NA,ncol(xs))
for (i in 1:ncol(xs)) a[i] <- length(unique(xs[,i]))
xcf <- matrix(NA,nrow(xs),sum(a))
b <- cumsum(a)
colnames(xcf) <- rep(colnames(xs),times=a)
colnum <- order(unique(xs[,1]))
for (i in 2:ncol(xs)) colnum <- c(colnum,order(unique(xs[,i])))
colnames(xcf) <- paste(colnames(xcf),colnum)
for (i in 1:nrow(xs))
{
  for (j in 1:length(a))
  {
    res <- rep(0, a[j])
    where <- match( xs[i,j], unique(xs[,j]) )
    res[ where ] <- 1 
    xcf[i,(max(b[j-1],0)+1):b[j]] <- res
  }
}
for (i in 1:length(b))
{
  ind <- match(xcf[,b[i]],1)==1
  xcf[ind,(max(b[i-1],0)+1):b[i]] <- -1
}
xcf <- xcf[,-b]
x.all <- xcf

# Identify label ijkl for yield
for (ijkl in 1:length(tt)) {
  add.i <- colnames(x.all)[which(x.all[ijkl,]!=0)][1]
  add.I <- sum(grepl("additive",colnames(x.all)[which(x.all[ijkl,]!=0)]))
  if (add.I>1) add.i <- "additive 0"
  ary.j <- colnames(x.all)[which(x.all[ijkl,]!=0)][1+add.I]
  ary.J <- sum(grepl("aryl_halide",colnames(x.all)[which(x.all[ijkl,]!=0)]))
  if (ary.J>1) ary.j <- "aryl_halide 0"
  bas.k <- colnames(x.all)[which(x.all[ijkl,]!=0)][1+add.I+ary.J]
  bas.K <- sum(grepl("base",colnames(x.all)[which(x.all[ijkl,]!=0)]))
  if (bas.K>1) bas.k <- "base 0"
  lig.l <- colnames(x.all)[which(x.all[ijkl,]!=0)][1+add.I+ary.J+bas.K]
  lig.L <- sum(grepl("ligand",colnames(x.all)[which(x.all[ijkl,]!=0)]))
  if (lig.L>1) lig.l <- "ligand 0"
  rownames(y)[ijkl] <- paste(add.i, ary.j, bas.k, lig.l, sep=":")
}
y.all <- y

# Mixed terms with 2-levels combinations
xx <- rep(1,nrow(xcf))
bb <- cumsum(a-1)
for (j in 1:3) {
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxp <- xcf[,i]*xcf[,-c(1:bb[j])]
    colnames(xxp) <- paste(colnames(xcf)[i],colnames(xcf[,-c(1:bb[j])]),sep=":")
    xx <- cbind(xx,xxp)
  }
}
xx <- cbind(xcf,xx[,-1])
xx.all <- xx

# Mixed terms with 3-levels combinations
xcf1 <- xcf
colnames(xcf1) <- c(rep("additive",21),rep("aryl_halide",14),rep("base",2),rep("ligand",3))
xx1 <- xx[,-c(1:40)]

xxx <- rep(1,nrow(xcf))
ind <- rep(TRUE,ncol(xx1))
for (j in 1:2) {
  ind <- as.logical((!grepl(colnames(xcf1)[bb[j]],colnames(xx1)))*(ind))
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxxp <- xcf[,i]*xx1[,ind]
    colnames(xxxp) <- paste(colnames(xcf)[i],colnames(xx1)[ind],sep=":")
    xxx <- cbind(xxx,xxxp)
  }
}
xxx <- cbind(xx,xxx[,-1])
xxx.all <- xxx

# Mixed terms with 4-levels combinations
xxx1 <- xxx[,-c(1:515)]

xxxx <- rep(NA,nrow(xcf))
for (i in 1:21) {
  xxxxp <- xcf[,i]*xxx1[,1597:1680]
  colnames(xxxxp) <- paste(colnames(xcf)[i],colnames(xxx1)[1597:1680],sep=":")
  xxxx <- cbind(xxxx,xxxxp)
}
xxxx <- cbind(xxx,xxxx[,-1])
xxxx.all <- xxxx



################################################################################
## Data Analysis on artificial ANOVA with two factors and two-way interactions  
################################################################################
X_only_main<-x.all

print(dim(y))
X_all_main<-xx.all ##have X_all_main for later
print(dim(X_all_main))

############################################ FUNCTIONS THAT I USE ######################################################################


checkBlocks <- function(mat, u, l) {
  if (u + l != nrow(mat) || nrow(mat) != ncol(mat)) {
    stop("Input matrix is not square or does not match u + l.")
  }
  
  # Check upper-left u x u block
  upper_left_block <- mat[1:u, 1:u]
  if (any(upper_left_block != 0)) {
    non_zero_positions <- which(upper_left_block != 0, arr.ind = TRUE)
    cat("Non-zero entries in upper-left block:\n")
    for (i in 1:nrow(non_zero_positions)) {
      cat("Position:", non_zero_positions[i, ])
    }
  } else {
    cat("No non-zero entries in the upper-left block.\n")
  }
  
  # Check lower-right l x l block
  lower_right_block <- mat[(nrow(mat) - l + 1):nrow(mat), (ncol(mat) - l + 1):ncol(mat)]
  if (any(lower_right_block != 0)) {
    non_zero_positions <- which(lower_right_block != 0, arr.ind = TRUE)
    cat("Non-zero entries in lower-right block:\n")
    for (i in 1:nrow(non_zero_positions)) {
      cat("Position:", non_zero_positions[i, ] + c((nrow(mat) - l + 1), (ncol(mat) - l + 1)))
    }
  } else {
    cat("No non-zero entries in the lower-right block.\n")
  }
}

# Example usage:
# Create a 5x5 square matrix
#example_matrix <- matrix(c(1, 10, 3, 0, 0,
                           #0, 2, 0, 0, 0,
                           #0, 0, 0, 0, 0,
                           #0, 4, 0, 10, 5,
                           #7, 7, 7, 10, 0), nrow = 5, byrow = TRUE)

# Check blocks with u = 2 and l = 3
#checkBlocks(example_matrix, 3, 2)


r2 <- function(actual, predicted) {
  # Calculate the mean of the actual values
  mean_actual <- mean(actual)
  
  # Calculate the total sum of squares
  total_sum_squares <- sum((actual - mean_actual)^2)
  
  # Calculate the residual sum of squares
  residual_sum_squares <- sum((actual - predicted)^2)
  
  # Calculate R-squared
  r_squared <- 1 - (residual_sum_squares / total_sum_squares)
  
  return(r_squared)
}





pairwise_product <- function(A, B) {
  # Get the number of columns in each matrix
  ncol_A <- ncol(A)
  ncol_B <- ncol(B)
  
  # Initialize an empty list to store the pairwise products
  products <- list()
  
  # Loop through all combinations of columns
  for (i in 1:ncol_A) {
    for (j in 1:ncol_B) {
      # Calculate the pairwise product and store it in the list
      products[[length(products) + 1]] <- A[, i] * B[, j]
    }
  }
  
  # Combine the pairwise products into a single matrix
  result <- do.call(cbind, products)
  
  return(result)
}

create_pairwise_interactions <- function(X, num_categories_list) {
  num_factors <- length(num_categories_list)
  num_cols <- (sum(num_categories_list)^2- sum(num_categories_list^2))/2 ## sum ab= [(sum a)^2- sum a^2]/2
  
  # Initialize the new matrix for pairwise interactions
  pairwise_interactions <- matrix(0, nrow = nrow(X), ncol = num_cols)
  col_idx_start <- 1
  
  # Iterate over all pairs of different factors
  for (i in 1:(num_factors - 1)) {
    for (j in (i + 1):num_factors) {
      # Get the number of categories for factor i and factor j
      num_categories_i <- num_categories_list[i]
      num_categories_j <- num_categories_list[j]
      col_idx_final<-col_idx_start+num_categories_i*num_categories_j-1 #substract the one where w start
      
      # Extract columns corresponding to factor i and factor j
      if (i==1)
      {cols_i<-(1:num_categories_list[i])}
      else
      {
        cols_i <- (sum(num_categories_list[1:(i - 1)]) + 1):(sum(num_categories_list[1:i])) }
      
      cols_j <- (sum(num_categories_list[1:(j - 1)]) + 1):(sum(num_categories_list[1:j]))
      print('cols i')
      print(cols_i)
      print('colsj')
      print(cols_j)
      # Compute the pairwise interactions between factor i and factor j
      print("matrix i")
      print(matrix(X[,cols_i], ncol = length(cols_i)))
      pairwise_interaction <- pairwise_product(matrix(X[,cols_i], ncol = length(cols_i)), matrix(X[,cols_j], ncol = length(cols_j))  )
      print('a')
      # Store the pairwise interactions in the new matrix
      pairwise_interactions[, col_idx_start:col_idx_final] <- pairwise_interaction
      
      # Increment the column index
      col_idx_start<-col_idx_final+1
    }
  }
  
  return(pairwise_interactions)
}








# Order the column names
#sorted_column_names <- sort(colnames(X_all_main))

# Reorder the columns of your data frame using the sorted column names
#X_all_main <- X_all_main[, sorted_column_names]
print(colnames((X_only_main)))





X_xx<-xx
X_all_main
colnames(X_xx)
class(X_xx)
sum(X_xx[,155])

all(X_xx[,37]*X_xx[,40]==X_xx[,515])
#################### PREP AND SPLIT DATA #######################################################################################################
#libraries

####### main data has only halide and ligand ###############
library(Metrics)

library(hierNet)
library(caret)
library(dplyr)
library(Metrics)










##Create interactions- maybe not needed###
create_interactions <- function(X) {
  n_features <- ncol(X)
  interaction_terms <- matrix(0, nrow = nrow(X), ncol = n_features * (n_features - 1) / 2)
  col_counter <- 1
  
  for (i in 1:(n_features - 1)) {
    for (j in (i + 1):n_features) {
      interaction_terms[, col_counter] <- X[, i] * X[, j]
      col_counter <- col_counter + 1
    }
  }
  
  return(cbind(X, interaction_terms))
}

X_train_interactions <- create_interactions(X_train)
X_test_interactions <- create_interactions(X_test)
###############



###FUNCTION TO COMPUTE ZEROS BEFORE AND AFTER###

zero_matrix_operation <- function(matrix_input, l1, l2, l3, l4, tol=1e-5) {
  # Calculate the number of zeros before the operation (excluding modified positions)
  zeros_before <- sum(abs(matrix_input) <= tol) - sum(abs(matrix_input[1:l1, 1:l1]) <= tol) -
    sum(abs(matrix_input[(l1 + 1):(l1 + l2), (l1 + 1):(l1 + l2)]) <=tol ) -
    sum(abs(matrix_input[(l1 + l2 + 1):(l1 + l2 + l3), (l1 + l2 + 1):(l1 + l2 + l3)]) <= tol) -
    sum(abs(matrix_input[(l1 + l2 + l3 + 1):nrow(matrix_input), (l1 + l2 + l3 + 1):ncol(matrix_input)]) <=tol )
  #zeros interested
  
  # Update elements in the matrix according to the provided indices
  matrix_input[1:l1, 1:l1] <- 0
  matrix_input[(l1 + 1):(l1 + l2), (l1 + 1):(l1 + l2)] <- 0
  matrix_input[(l1 + l2 + 1):(l1 + l2 + l3), (l1 + l2 + 1):(l1 + l2 + l3)] <- 0
  matrix_input[(l1 + l2 + l3 + 1):nrow(matrix_input), (l1 + l2 + l3 + 1):ncol(matrix_input)] <- 0
  
  # Calculate the number of zeros after the operation
  zeros_after <- sum(abs(matrix_input) <= tol) #total 0s
  
  # Return the number of zeros before the operation
  return(list(zeros_before, zeros_after))
}

# Example usage:
# Create a 5x5 matrix
matrix_input <- matrix(0:24, nrow = 5)

# Call the function with the matrix and specified lengths
result <- zero_matrix_operation(matrix_input, 1, 1, 2, 1)

# Print the result
print(result)




#####################DO the analysis on whole data with pairwise interacions- Data PREP################################

index <- createDataPartition(y = y$yield, p = 0.7, list = FALSE)



# Separate X_train, X_test, y_train, y_test
X_all_train <- X_only_main[index, ]
y_train <- y[index, ]
X_all_test <- X_only_main[-index,]
y_test <- y[-index, ]



y_train<-y_train*100
y_test<-y_test*100

max(y_train)

print(dim(X_all_train))

print(dim(X_all_test))
print(length(y_train))
print(length(y_test))


colnames(X_all_train)


######################## HIER NET ##########################################################

lamhat=100
fit=hierNet(X_all_train,y_train, lam = lamhat, diagonal = FALSE)
yhat_test=predict(fit,X_all_test)
yhat_train=predict(fit,X_all_train)
print("-----Library hiernet-----")
print(paste("rmse train:",rmse(y_train, yhat_train)))
print(paste("r2 train:", r2(y_train, yhat_train)))
print(paste("rmse test:",rmse(y_test, yhat_test)))
print(paste("r2 test:", r2(y_test, yhat_test)))
zero_matrix_operation(fit$th,21,14,2,3)
print(fit$th)
options(max.print=1e6)
print(fit)
print(dim(fit$th))
#checkBlocks(fit$th,15,4)
fit$th
fit$lam
max(yhat)
max(y_test)
print("------------")
y_train
################## DO THE SAME WITH MY HIERNET

source("WeakHierNet_Class_corrected_unscaled.R")
print('My weakhiernet')

t<-6e-5+3e-8
#t<-0.001
colnames(X_only_main)
y_all_centered<-scale(y$yield*100, center = TRUE, scale = FALSE)
myWeakHierNet<-WeakHierNetUnscaled (X=X_only_main, Beta_plus_init=matrix(0,nrow = dim(X_only_main)[2], ncol = 1), Beta_minus_init=matrix(0,nrow = dim(X_only_main)[2], ncol = 1), 
              Theta_init=matrix(0, ncol = dim(X_only_main)[2], nrow = dim(X_only_main)[2]), y=y_all_centered, lambda=10, t=t, tol=1e-7, 
              max_iter=10000, eps=1e-8)  #Increase max iter if needed or decrease tol 

# Fit the model
fitted=myWeakHierNet$fit(X=X_only_main, Beta_plus_init=matrix(0,nrow = dim(X_only_main)[2], ncol = 1), Beta_minus_init=matrix(0,nrow = dim(X_only_main)[2], ncol = 1), 
                         Theta_init=matrix(0, ncol = dim(X_only_main)[2], nrow = dim(X_only_main)[2]), y=y_all_centered, lambda=10, t=t, tol=1e-7, 
                         max_iter=10000, eps=1e-8 )

# Make predictions
new_X <- X_only_main
print("R2 score on all data")
myWeakHierNet$R2_score(self=fitted, new_X= as.matrix(X_only_main), y_true = y_all_centered, verbose = TRUE)

fitted
y_pred<-myWeakHierNet$predict(self=fitted, X_all_test)

plot(y_pred, scale(y_test, scale= FALSE))

sum(abs(fitted$Theta_hat)<=0.000001)


###se the nr of 0s
zero_matrix_operation(fitted$Theta_hat,21,14,2,3,0)
fitted$Theta_hat

# Call the function to store the vectors and theta in a file
folder_name <- "Results"
file_name <- "lmd100_it10k"
store_vectors_theta(fitted$Beta_hat_plus, fitted$Beta_hat_minus, fitted$Theta_hat, folder_name, file_name)

# Example usage:
folder_name <- "Results"
file_name <- "lmd80_it10k"
data <- read_vectors_theta(folder_name, file_name)
theta_lmd1 <- data$theta

lm1_idx=which(theta_lmd1==0)
lm2_idx=which(fitted$Theta_hat==0)
length(intersect(lm1_idx, lm2_idx))
length(lm1_idx)
length(lm2_idx)



##### SEE ALSO LM############
X<-X_all_train
p <- ncol(X)
n<-nrow(X)
X <- scale(X)#standardize X
#CONSTRUCT Z
Z=t(apply(X, 1, function(x) outer(x, x, "*")))
Z <- scale(Z, center = TRUE, scale = FALSE) #center Z
Z=set_Z_values_to_zero(Z,p) #MAKE 0 cols for Xi Xi interaction
y<-y_train
y <- scale(y, center = TRUE, scale = FALSE) #center y
XZ <- cbind(X, Z)
dim(XZ)

# Identify columns with all zeros

print(dim(zero_columns))
# Remove constant columns
XZ<-XZ[, colSums(XZ != 0) > 0]
print(dim(XZ))
print(XZ[23:30,55:100])

# Perform linear regression
model <- lm(y ~ XZ)
coefficients<-coef(model)
coefficients
coefficients_sub <- coefficients[41:length(coefficients)]

# Reshape coefficients into a 40x40 matrix
coeff_matrix <- matrix(coefficients_sub, nrow = 40, ncol = 40, byrow = TRUE)
zero_matrix_operation(coeff_matrix,21,14,2,3,0)
coeff_matrix


###########################################################################################################





############################# USE MY CORRECTED CLASS ##########################################
source("WeakHierNet_Class_corrected.R")

Beta_plus_init=matrix(0,nrow = dim(X_all_train)[2], ncol = 1)
Beta_minus_init=matrix(0,nrow = dim(X_all_train)[2], ncol = 1)
Theta_init=matrix(0, ncol = dim(X_all_train)[2], nrow = dim(X_all_train)[2])
lambda=80
t=0.00001
tol = 1e-5

myWeakHierNet<-WeakHierNet (X=X_all_train, Beta_plus_init= Beta_plus_init, Beta_minus_init=Beta_minus_init, 
                            Theta_init = Theta_init, y=y_train, lambda=lambda, t=t, tol=tol, 
                            max_iter=100, eps=1e-8 ) 



# Fit the model
fitted=myWeakHierNet$fit(X=X_all_train, y=y_train, lambda=lambda, t = t, tol = tol, max_iter = 100, eps = 1e-8,
                         Beta_plus_init = Beta_plus_init,Beta_minus_init =  Beta_minus_init, Theta_init =  Theta_init)
fitted
# Make predictions
#y_pred=myWeakHierNet$predict(fitted, X_all_train)
#y_pred
#scale(y_train,center = TRUE, scale= FALSE)

myWeakHierNet$R2_score(self=fitted, new_X= as.matrix(X_all_test), y_true = y_test, verbose = TRUE)

###########################################################################################################



###CONCLUSIONS 

##It seems that there are no problems due to considering all pairs interactions bc for high enough lambda tho ones that should be 0 by default go to 0
##It has around 0.56 r2 score on both training and test set and rmse around 0.18
## It works better than a linear reg that does not consider interactions. It works similar to linear reg with all interactions. 
## It works much better on the entire data (around r2 = 0.856)


##de facut !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

###vezi daca Z trebuie scaled in ambele modele !!!!!!!!!!!!!!!!!!!!!!!!!!




################## USE CAP WITH GENERAL OPTIMIZATION on all 4 main and 2 interactions #####################
#prep data

source("CAP_general_optim_functions.R")




#Create only interaction indexes. X is random, not used the one from create interactions since it it already created

p<-ncol(X_all_train)
p

X_train_cap<-X_all_train
beta_interactions<-create_all_interactions(X_all_train[,c(1:40)],num_categories_list=c(21,14,2,3), main_lengths=40)$beta_interactions
y_train_cap<-y_train
y_scaled_train<-scale(y_train, scale=FALSE)

X_test_cap<-X_all_test
y_test_cap<-y_test
y_scaled_test<-scale(y_test,scale=FALSE)

print(beta_interactions[3,])

print(dim(X_train_cap))
#use the minimizer


# Initial guess for beta
#initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 1)
initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 1)

lambda<-0.2# SOmetimes works a bit better with scaled X
### USE OPTIM FUNCTION /nlm
# Call optim to minimize the function
opt_result <- optim(par = initial_beta, fn = cap,gr=gradient_cap, X = scale(X_train_cap), y = y_scaled_train, lambda=lambda,beta_interactions = beta_interactions, method="BFGS",control = list(maxit = 100))
#opt_result<-nlm(f = function(beta) cap(beta, X_train_cap, y_scaled_train, beta_interactions, lambda = lambda), p = initial_beta, iterlim = 12)
#opt_result
# Extract the optimal beta
optimal_beta <- opt_result$par
#optimal_beta<-opt_result$estimate


#print("Using optim:")
print("beta")
print(optimal_beta)
print("r2 train")
print(r2(y_scaled_train,scale(X_train_cap)%*%optimal_beta))
print("r2 test")
print(r2(y_scaled_test, scale(X_test_cap)%*%optimal_beta))



#####USE NLOPT#########
library(nloptr)
# Define optimization problem
lambda<-1
#opt_problem <- nloptr(
  #x0 = initial_beta,
  #eval_f = function(beta) cap(beta, X_train_cap, y_scaled_train, beta_interactions, lambda),
  #eval_grad_f= function(beta) gradient_cap(beta, X_train_cap, y_scaled_train, beta_interactions, lambda),
  #opts = list(algorithm = "NLOPT_LD_LBFGS_NOCEDAL", maxeval = 800)
#)
# Extract the optimal parameters (beta)
#optimal_beta <- opt_problem$solution

#print("Using optim:")



# Get the optimized parameters
optimized_beta <- result$target$values()[[1]]

# Print the optimized parameters
print(optimized_beta)
print("beta")
print(optimal_beta)
print("r2 train")
print(r2(y_scaled_train,X_train_cap%*%optimal_beta))
print("r2 test")
print(r2(y_scaled_test, X_test_cap%*%optimal_beta))


#Use nlminb#############
# Define your function

# Set initial values
#initial_beta <- rep(1, ncol(X_train_cap))  # Assuming X is your design matrix
#initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 1)

#lambda <- 0.1  # Your lambda value for regularization

# Define lower and upper bounds for beta
#lower_bound <- rep(-1000, length(initial_beta))
#upper_bound <- rep(1000, length(initial_beta))

# Define control parameters
#control_params <- list(iter.max = 200,step.min=10, step.max=100)  # Change maxeval to your desired maximum number of function evaluations

# Optimize using nlminb()
#opt_result <- nlminb(start = initial_beta, objective = cap,gradient=gradient_cap, X = X_train_cap, y = y_scaled_train, beta_interactions = beta_interactions, lambda = lambda, lower = lower_bound, upper = upper_bound, control = control_params)

# Extract the optimized beta values
#optimal_beta <- opt_result$par


print("Using optim:")
print("beta")
print(optimal_beta)
print("r2 train")
print(r2(y_scaled_train,X_train_cap%*%optimal_beta))
print("r2 test")
print(r2(y_scaled_test, X_test_cap%*%optimal_beta))
#print(X_all_train%*%optimal_beta)
######





#### USE CVRX #################

#suppressWarnings(library(CVXR, warn.conflicts=FALSE))


#lambda<-0
# Use cvxr to optimize beta
#beta <- Variable(ncol(X_train_cap))

#problem <- Problem(Minimize(cap(beta, X_train_cap, y_train, beta_interactions ,lambda=lambda)))
#result <- solve(problem,control = list(max_iters = 10))

# Get the optimized beta values
#optimized_beta <- result$getValue(beta)
#print("Using cvrx:")
#print(optimized_beta)

#y_train_pred=X_train_cap%*%optimal_beta
#print(r2(y_train, y_train_pred))




########## USe multiroot on gradient################
# Define the function
#library(rootSolve)

#initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 2)

#lambda<-200

# Find the root
#optimal_beta <- multiroot(gradient_cap, start = initial_beta, X=scale(X_train_cap) ,y= y_scaled_train ,
                  #beta_interactions = beta_interactions, lambda = lambda, maxiter=10)
#print("beta")
#print(optimal_beta)

#optimal_beta<-optimal_beta$f.root

#print("r2 train")
#print(r2(y_scaled_train,scale(X_train_cap)%*%optimal_beta))
#print("r2 test")
#print(r2(y_scaled_test, scale(X_test_cap)%*%optimal_beta))








########################### USE ALSO CAPGENERAL ONLY on 2 mains #######################################################


colnames(X_all_train)

target_ligand <- "ligand"

# Use grep to find the indices of columns that contain the specified ligand name
matching_indices <- grep(paste0("aryl_halide.*", target_ligand), colnames((X_all_train)))

# Output the matching indices
print(matching_indices)

X_train<-cbind(X_all_train[,c(22:35)], X_all_train[,c(38:40)], X_all_train[,matching_indices])
X_test<-cbind(X_all_test[,c(22:35)], X_all_test[,c(38:40)], X_all_test[,matching_indices])
print(colnames(X_train))
p<-ncol(X_train)
p

X_train_cap<-X_train

beta_interactions<-create_all_interactions(X_train_cap[,c(1:17)], num_categories_list = c(14,3), main_lengths=17)$beta_interactions # X train is not imp
y_train_cap<-y_train
y_scaled_train<-scale(y_train, scale=FALSE)

X_test_cap<-X_test
y_test_cap<-y_test
y_scaled_test<-scale(y_test,scale=FALSE)


print(dim(X_train_cap))
#use the minimizer

# Initial guess for beta
#initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 1)
initial_beta <-rnorm(ncol(X_train_cap), mean = 0, sd = 1)

lambda<-0.2
### USE OPTIM FUNCTION /nlm
# Call optim to minimize the function
opt_result <- optim(par = initial_beta,gr=gradient_cap, fn = cap, X = scale(X_train_cap), y = y_scaled_train, lambda=lambda,beta_interactions = beta_interactions, method="BFGS",control = list(maxit = 100))
#opt_result<-nlm(f = function(beta) cap(beta, X_train_cap, y_scaled_train, beta_interactions, lambda = lambda), p = initial_beta, iterlim = 12)
#opt_result
# Extract the optimal beta
optimal_beta <- opt_result$par
#optimal_beta<-opt_result$estimate


#print("Using optim:")
print("beta")
print(optimal_beta)
print("r2 train")
print(r2(y_scaled_train,scale(X_train_cap)%*%optimal_beta))
print("r2 test")
print(r2(y_scaled_test, scale(X_test_cap)%*%optimal_beta))











# Load necessary library for linear regression####################################################################
library(stats)
# Assuming X_all_train is your feature matrix and y_train is your target vector
# Convert your data into a data frame
data <- data.frame(X_train_cap, y_train)

# Fit a linear regression model
lm_model <- lm(y_train ~ ., data)

# Predict the target values
predictions <- predict(lm_model, data)

# Compute R-squared score
r_squared <- summary(lm_model)$r.squared

print(r_squared)
coef(lm_model)








store_vectors_theta <- function(beta_plus, beta_minus, theta, folder_name, file_name) {
  # Create the full file path
  file_path <- file.path(folder_name, paste0(file_name, ".txt"))
  
  # Write beta_plus, beta_minus, and theta to the file
  write.table(beta_plus, file = file_path, append = FALSE, col.names = FALSE, row.names = FALSE)
  write.table(beta_minus, file = file_path, append = TRUE, col.names = FALSE, row.names = FALSE)
  write.table(theta, file = file_path, append = TRUE, col.names = FALSE, row.names = FALSE)
}

# Example usage:
# Define sample data for beta_plus, beta_minus, and theta
beta_plus <- c(1, 2, 3)
beta_minus <- c(4, 5, 6)
theta <- matrix(1:9, nrow = 3, ncol = 3)

# Call the function to store the vectors and theta in a file
folder_name <- "Results"
file_name <- "initial"
store_vectors_theta(beta_plus, beta_minus, theta, folder_name, file_name)


read_vectors_theta <- function(folder_name, file_name) {
  # Create the full file path
  file_path <- file.path(folder_name, paste0(file_name, ".txt"))
  
  # Read data from the file
  data <- scan(file_path)
  
  # Split the data into beta_plus, beta_minus, and theta
  p <- sqrt(length(data)+1)-1 ## there are p^2 +2p elems
  print(p)
  beta_plus <- as.vector(data[1:p])
  beta_minus <- as.vector(data[(p + 1):(2 * p)])
  theta <- matrix(data[(2 * p + 1):length(data)], ncol = p)
  
  return(list(beta_plus = beta_plus, beta_minus = beta_minus, theta = theta))
}

# Example usage:
folder_name <- "Results"
file_name <- "initial"
data <- read_vectors_theta(folder_name, file_name)

# Access the vectors and matrix
beta_plus <- data$beta_plus
beta_minus <- data$beta_minus
theta <- data$theta


beta_plus
beta_minus
theta











